{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_windows(df, window_size, stride):\n",
    "    # 입력 유효성 검사\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise ValueError(\"Input must be a pandas DataFrame\")\n",
    "    if not isinstance(window_size, int) or window_size <= 0:\n",
    "        raise ValueError(\"window_size must be a positive integer\")\n",
    "    if not isinstance(stride, int) or stride <= 0:\n",
    "        raise ValueError(\"stride must be a positive integer\")\n",
    "    \n",
    "    # 결과를 저장할 딕셔너리 생성\n",
    "    result_dict = {}\n",
    "    \n",
    "    # 각 열에 대해 반복\n",
    "    for column in df.columns:\n",
    "        # 시간 윈도우 생성\n",
    "        windows = {}\n",
    "        for i in range(0, len(df) - window_size + 1, stride):\n",
    "            window = df[column].iloc[i:i+window_size].values\n",
    "            windows[df.index[i+window_size-1]] = window\n",
    "        \n",
    "        # 윈도우 데이터로 새 DataFrame 생성\n",
    "        window_df = pd.DataFrame.from_dict(windows, orient='index')\n",
    "        window_df.columns = [f\"{column}_t-{window_size-i-1}\" for i in range(window_size)]\n",
    "        \n",
    "        # 결과 딕셔너리에 추가\n",
    "        result_dict[column] = window_df\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(r\"D:\\Workspace\\DnS\\data\\AJ네트웍스_20190825_20240825.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1233, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5  # window 크기\n",
    "stride = 2  # stride 크기\n",
    "df_list = create_time_windows(df, window_size, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1233, 6)\n",
      "Index(['시가', '고가', '저가', '종가', '거래량', '등락률'], dtype='object')\n",
      "              시가    고가    저가    종가    거래량       등락률\n",
      "날짜                                                 \n",
      "2019-08-26  4615  4615  4480  4540  34971 -2.365591\n",
      "2019-08-27  4505  4585  4505  4560  20983  0.440529\n",
      "2019-08-28  4540  4640  4490  4580  20526  0.438596\n",
      "2019-08-29  4650  4650  4365  4650  22742  1.528384\n",
      "2019-08-30  4645  4715  4610  4700  20754  1.075269\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>종가_t-4</th>\n",
       "      <th>종가_t-3</th>\n",
       "      <th>종가_t-2</th>\n",
       "      <th>종가_t-1</th>\n",
       "      <th>종가_t-0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-08-30</th>\n",
       "      <td>4540</td>\n",
       "      <td>4560</td>\n",
       "      <td>4580</td>\n",
       "      <td>4650</td>\n",
       "      <td>4700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-03</th>\n",
       "      <td>4580</td>\n",
       "      <td>4650</td>\n",
       "      <td>4700</td>\n",
       "      <td>4670</td>\n",
       "      <td>4590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-05</th>\n",
       "      <td>4700</td>\n",
       "      <td>4670</td>\n",
       "      <td>4590</td>\n",
       "      <td>4575</td>\n",
       "      <td>4440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-09</th>\n",
       "      <td>4590</td>\n",
       "      <td>4575</td>\n",
       "      <td>4440</td>\n",
       "      <td>4455</td>\n",
       "      <td>4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-11</th>\n",
       "      <td>4440</td>\n",
       "      <td>4455</td>\n",
       "      <td>4375</td>\n",
       "      <td>4385</td>\n",
       "      <td>4395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-12</th>\n",
       "      <td>4320</td>\n",
       "      <td>4315</td>\n",
       "      <td>4275</td>\n",
       "      <td>4320</td>\n",
       "      <td>4360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-14</th>\n",
       "      <td>4275</td>\n",
       "      <td>4320</td>\n",
       "      <td>4360</td>\n",
       "      <td>4325</td>\n",
       "      <td>4350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-19</th>\n",
       "      <td>4360</td>\n",
       "      <td>4325</td>\n",
       "      <td>4350</td>\n",
       "      <td>4410</td>\n",
       "      <td>4400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-21</th>\n",
       "      <td>4350</td>\n",
       "      <td>4410</td>\n",
       "      <td>4400</td>\n",
       "      <td>4500</td>\n",
       "      <td>4620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-23</th>\n",
       "      <td>4400</td>\n",
       "      <td>4500</td>\n",
       "      <td>4620</td>\n",
       "      <td>4610</td>\n",
       "      <td>4620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>615 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            종가_t-4  종가_t-3  종가_t-2  종가_t-1  종가_t-0\n",
       "2019-08-30    4540    4560    4580    4650    4700\n",
       "2019-09-03    4580    4650    4700    4670    4590\n",
       "2019-09-05    4700    4670    4590    4575    4440\n",
       "2019-09-09    4590    4575    4440    4455    4375\n",
       "2019-09-11    4440    4455    4375    4385    4395\n",
       "...            ...     ...     ...     ...     ...\n",
       "2024-08-12    4320    4315    4275    4320    4360\n",
       "2024-08-14    4275    4320    4360    4325    4350\n",
       "2024-08-19    4360    4325    4350    4410    4400\n",
       "2024-08-21    4350    4410    4400    4500    4620\n",
       "2024-08-23    4400    4500    4620    4610    4620\n",
       "\n",
       "[615 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list['종가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lag_0</th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_2</th>\n",
       "      <th>lag_3</th>\n",
       "      <th>lag_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-08-30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.403125</td>\n",
       "      <td>-0.153571</td>\n",
       "      <td>-0.403348</td>\n",
       "      <td>-0.346205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-03</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.046442</td>\n",
       "      <td>-0.579401</td>\n",
       "      <td>-0.227715</td>\n",
       "      <td>0.260674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-05</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.260341</td>\n",
       "      <td>-0.030414</td>\n",
       "      <td>-0.333942</td>\n",
       "      <td>-0.395985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-09</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.293466</td>\n",
       "      <td>-0.070114</td>\n",
       "      <td>-0.385350</td>\n",
       "      <td>-0.338002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.205000</td>\n",
       "      <td>-0.330000</td>\n",
       "      <td>-0.285000</td>\n",
       "      <td>-0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>-0.522865</td>\n",
       "      <td>-0.033609</td>\n",
       "      <td>0.023140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010069</td>\n",
       "      <td>-0.208696</td>\n",
       "      <td>-0.021281</td>\n",
       "      <td>-0.280092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.343426</td>\n",
       "      <td>-0.442629</td>\n",
       "      <td>-0.345219</td>\n",
       "      <td>-0.055578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.269285</td>\n",
       "      <td>-0.116328</td>\n",
       "      <td>-0.269373</td>\n",
       "      <td>-0.383583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.322917</td>\n",
       "      <td>-0.223958</td>\n",
       "      <td>-0.325521</td>\n",
       "      <td>-0.273438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>615 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            lag_0     lag_1     lag_2     lag_3     lag_4\n",
       "2019-08-30    1.0  0.403125 -0.153571 -0.403348 -0.346205\n",
       "2019-09-03    1.0  0.046442 -0.579401 -0.227715  0.260674\n",
       "2019-09-05    1.0  0.260341 -0.030414 -0.333942 -0.395985\n",
       "2019-09-09    1.0  0.293466 -0.070114 -0.385350 -0.338002\n",
       "2019-09-11    1.0  0.205000 -0.330000 -0.285000 -0.090000\n",
       "...           ...       ...       ...       ...       ...\n",
       "2024-08-12    1.0  0.033333 -0.522865 -0.033609  0.023140\n",
       "2024-08-14    1.0  0.010069 -0.208696 -0.021281 -0.280092\n",
       "2024-08-19    1.0  0.343426 -0.442629 -0.345219 -0.055578\n",
       "2024-08-21    1.0  0.269285 -0.116328 -0.269373 -0.383583\n",
       "2024-08-23    1.0  0.322917 -0.223958 -0.325521 -0.273438\n",
       "\n",
       "[615 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from acf import calculate_acf\n",
    "acf_df = calculate_acf(df_list['종가'], window_size=window_size)\n",
    "acf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from buffett import calculate_buffett_index\n",
    "buffett = calculate_buffett_index(df['종가'], 'KOR')\n",
    "buffett_df = create_time_windows(buffett.to_frame(), window_size, stride)['종가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deMartini import demartini_index\n",
    "de = demartini_index(df['종가'])\n",
    "de_df = create_time_windows(de.to_frame(), window_size, stride)['rsi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from div_each_before import div_each_before\n",
    "\n",
    "deb = div_each_before(df['종가'])\n",
    "deb_df = create_time_windows(deb.to_frame(), window_size, stride)['종가']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fractional_difference import fractional_difference\n",
    "fracdiff = fractional_difference(df['종가'], 0.3)\n",
    "fracdiff_df = create_time_windows(fracdiff.to_frame(), window_size, stride)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pivot import calculate_pivot_points\n",
    "pivot_points = calculate_pivot_points(df['고가'], df['저가'], df['종가'])\n",
    "pivot_points_df = create_time_windows(pivot_points, window_size, stride)['Pivot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sonar import sonar_indicator\n",
    "sn = sonar_indicator(df, window_size=14)\n",
    "sn_df = create_time_windows(sn.to_frame(), window_size, stride)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stocastic import stochastic_fast, stochastic_slow\n",
    "stfa = stochastic_fast(df)\n",
    "stsl = stochastic_slow(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastk_df = create_time_windows(stfa['fastk'].to_frame(), window_size, stride)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastd_df = create_time_windows(stfa['fastd'].to_frame(), window_size, stride)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "slowk_df = create_time_windows(stsl['slowk'].to_frame(), window_size, stride)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "slowd_df = create_time_windows(stsl['slowd'].to_frame(), window_size, stride)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(615, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slowd_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time_delay import time_delay_embedding\n",
    "time_delay_df = time_delay_embedding(df['종가'], 154, 5)[:615 :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(615, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_delay_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vix import calculate_vix\n",
    "calVix = calculate_vix(df['종가'], window_size)\n",
    "calVix_df = create_time_windows(calVix.to_frame(), window_size, stride)['종가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "날짜\n",
      "2019-08-26   -55.555556\n",
      "2019-08-27   -40.740741\n",
      "2019-08-28   -37.500000\n",
      "2019-08-29    -0.000000\n",
      "2019-08-30    -4.285714\n",
      "                ...    \n",
      "2024-08-19   -21.052632\n",
      "2024-08-20    -7.142857\n",
      "2024-08-21   -18.918919\n",
      "2024-08-22   -24.242424\n",
      "2024-08-23   -22.222222\n",
      "Length: 1233, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from williams import williams_r\n",
    "will = williams_r(df, 5) \n",
    "will_df = create_time_windows(will.to_frame(), window_size, stride)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of autoencoder input: (615, 14, 5)\n"
     ]
    }
   ],
   "source": [
    "def prepare_autoencoder_data(df_list, additional_dfs):\n",
    "    \"\"\"\n",
    "    Prepare data for autoencoder by combining multiple indicators.\n",
    "    \n",
    "    :param df_list: Dictionary of DataFrames from create_time_windows function\n",
    "    :param additional_dfs: List of additional DataFrames to include\n",
    "    :return: numpy array ready for autoencoder input\n",
    "    \"\"\"\n",
    "    # Get the shape of the first DataFrame to determine the number of samples and features\n",
    "    first_df = next(iter(df_list.values()))\n",
    "    n_samples, n_features = first_df.shape\n",
    "    \n",
    "    # Calculate the total number of indicators\n",
    "    n_indicators = len(df_list) + len(additional_dfs)\n",
    "    \n",
    "    # Initialize the result array\n",
    "    result = np.zeros((n_samples, n_indicators, n_features))\n",
    "    \n",
    "    # Fill in the data from df_list\n",
    "    for i, df in enumerate(df_list.values()):\n",
    "        result[:, i, :] = df.values\n",
    "    \n",
    "    # Fill in the data from additional_dfs\n",
    "    for i, df in enumerate(additional_dfs, start=len(df_list)):\n",
    "        result[:, i, :] = df.values\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Usage example:\n",
    "df_list = {\n",
    "    'acf': acf_df,\n",
    "    'buffett': buffett_df,\n",
    "    'demartini': de_df,\n",
    "    'div_each_before': deb_df,\n",
    "    'fractional_diff': fracdiff_df,\n",
    "    'pivot': pivot_points_df,\n",
    "    'sonar': sn_df,\n",
    "    'vix': calVix_df,\n",
    "    'williams': will_df\n",
    "}\n",
    "\n",
    "additional_dfs = [\n",
    "    fastk_df,\n",
    "    fastd_df,\n",
    "    slowk_df,\n",
    "    slowd_df,\n",
    "    time_delay_df  # Note: This might need adjustment if the shape doesn't match\n",
    "]\n",
    "\n",
    "# Prepare the data\n",
    "autoencoder_input = prepare_autoencoder_data(df_list, additional_dfs)\n",
    "\n",
    "print(f\"Shape of autoencoder input: {autoencoder_input.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by MinMaxScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 91\u001b[0m\n\u001b[0;32m     82\u001b[0m additional_dfs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     83\u001b[0m     fastk_df,\n\u001b[0;32m     84\u001b[0m     fastd_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m     time_delay_df\n\u001b[0;32m     88\u001b[0m ]\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Prepare the data with MinMax scaling applied to each column independently and NaN rows removed\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m autoencoder_input \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_autoencoder_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_dfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of autoencoder input: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mautoencoder_input\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[73], line 59\u001b[0m, in \u001b[0;36mprepare_autoencoder_data\u001b[1;34m(df_list, additional_dfs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Fill in the data from df_list and apply scaling\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df_list\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m---> 59\u001b[0m     scaled_df \u001b[38;5;241m=\u001b[39m \u001b[43mapply_minmax_scaling_to_each_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     result[:, i, :] \u001b[38;5;241m=\u001b[39m scaled_df\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Fill in the data from additional_dfs and apply scaling\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[73], line 15\u001b[0m, in \u001b[0;36mapply_minmax_scaling_to_each_column\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     13\u001b[0m df_scaled \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 15\u001b[0m     df_scaled[column] \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_scaled\n",
      "File \u001b[1;32mc:\\Users\\kimso\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\kimso\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\kimso\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:450\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kimso\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kimso\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:490\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    487\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m    489\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 490\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_pass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_array_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m data_min \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    498\u001b[0m data_max \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kimso\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\kimso\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1072\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m   1071\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m-> 1072\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1073\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1075\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m   1076\u001b[0m         )\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1079\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by MinMaxScaler."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def apply_minmax_scaling_to_each_column(df):\n",
    "    \"\"\"\n",
    "    Apply MinMax scaling to each column of a DataFrame independently.\n",
    "    \n",
    "    :param df: Input DataFrame\n",
    "    :return: DataFrame with each column scaled independently\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled = df.copy()\n",
    "    for column in df.columns:\n",
    "        df_scaled[column] = scaler.fit_transform(df[[column]])\n",
    "    return df_scaled\n",
    "\n",
    "def prepare_autoencoder_data(df_list, additional_dfs):\n",
    "    \"\"\"\n",
    "    Prepare data for autoencoder by combining multiple indicators.\n",
    "    \n",
    "    :param df_list: Dictionary of DataFrames from create_time_windows function\n",
    "    :param additional_dfs: List of additional DataFrames to include\n",
    "    :return: numpy array ready for autoencoder input\n",
    "    \"\"\"\n",
    "    # Get the shape of the first DataFrame to determine the number of samples and features\n",
    "    first_df = next(iter(df_list.values()))\n",
    "    n_samples, n_features = first_df.shape\n",
    "    \n",
    "    # Calculate the total number of indicators\n",
    "    n_indicators = len(df_list) + len(additional_dfs)\n",
    "    \n",
    "    # Initialize the result array\n",
    "    result = np.zeros((n_samples, n_indicators, n_features))\n",
    "    \n",
    "    # Fill in the data from df_list and apply scaling\n",
    "    for i, df in enumerate(df_list.values()):\n",
    "        scaled_df = apply_minmax_scaling_to_each_column(df)\n",
    "        result[:, i, :] = scaled_df.values\n",
    "    \n",
    "    # Fill in the data from additional_dfs and apply scaling\n",
    "    for i, df in enumerate(additional_dfs, start=len(df_list)):\n",
    "        scaled_df = apply_minmax_scaling_to_each_column(df)\n",
    "        result[:, i, :] = scaled_df.values\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Usage example:\n",
    "df_list = {\n",
    "    'acf': acf_df,\n",
    "    'buffett': buffett_df,\n",
    "    'demartini': de_df,\n",
    "    'div_each_before': deb_df,\n",
    "    'fractional_diff': fracdiff_df,\n",
    "    'pivot': pivot_points_df,\n",
    "    'sonar': sn_df,\n",
    "    'vix': calVix_df,\n",
    "    'williams': will_df\n",
    "}\n",
    "\n",
    "additional_dfs = [\n",
    "    fastk_df,\n",
    "    fastd_df,\n",
    "    slowk_df,\n",
    "    slowd_df,\n",
    "    time_delay_df\n",
    "]\n",
    "\n",
    "# Prepare the data with MinMax scaling applied to each column independently\n",
    "autoencoder_input = prepare_autoencoder_data(df_list, additional_dfs)\n",
    "\n",
    "print(f\"Shape of autoencoder input: {autoencoder_input.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.92884299, 0.42138562, 0.09838865, 0.17300311],\n",
       "        [0.40257649, 0.40342679, 0.40901771, 0.41744548, 0.42834138],\n",
       "        [       nan,        nan,        nan,        nan,        nan],\n",
       "        ...,\n",
       "        [0.43929593, 0.52538647, 0.55559695, 0.7616876 , 0.88114618],\n",
       "        [0.4498144 , 0.48442208, 0.51631447, 0.6174826 , 0.74696515],\n",
       "        [0.53319502, 0.12769784, 0.16759156, 0.4721346 , 0.38380652]],\n",
       "\n",
       "       [[0.        , 0.65471183, 0.0809289 , 0.2776527 , 0.9614132 ],\n",
       "        [0.40901771, 0.41744548, 0.42834138, 0.42056075, 0.41062802],\n",
       "        [       nan,        nan,        nan,        nan,        nan],\n",
       "        ...,\n",
       "        [0.55559695, 0.7616876 , 0.88114618, 0.97538368, 0.83442737],\n",
       "        [0.51631447, 0.6174826 , 0.74696515, 0.88084358, 0.91435649],\n",
       "        [0.5373444 , 0.14118705, 0.15871254, 0.46582545, 0.35015773]],\n",
       "\n",
       "       [[0.        , 0.8191051 , 0.51985207, 0.16923023, 0.10833279],\n",
       "        [0.42834138, 0.42056075, 0.41062802, 0.40576324, 0.38647343],\n",
       "        [       nan,        nan,        nan,        nan,        nan],\n",
       "        ...,\n",
       "        [0.88114618, 0.97538368, 0.83442737, 0.71418533, 0.47584043],\n",
       "        [0.74696515, 0.88084358, 0.91435649, 0.84882653, 0.6877534 ],\n",
       "        [0.54149378, 0.10791367, 0.1509434 , 0.47844374, 0.36908517]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.        , 0.88296115, 0.19027951, 0.15771956, 0.55056369],\n",
       "        [0.37359098, 0.36682243, 0.37198068, 0.38006231, 0.38003221],\n",
       "        [0.51024211, 0.47785666, 0.50753417, 0.54947884, 0.55018003],\n",
       "        ...,\n",
       "        [0.57622518, 0.61445102, 0.62420039, 0.66284836, 0.72248293],\n",
       "        [0.57154335, 0.58539722, 0.61655237, 0.63759045, 0.68281566],\n",
       "        [0.71161826, 0.76258993, 0.13096559, 0.32912723, 0.16088328]],\n",
       "\n",
       "       [[0.        , 0.82597942, 0.45116201, 0.23513344, 0.12444449],\n",
       "        [0.37198068, 0.38006231, 0.38003221, 0.394081  , 0.41545894],\n",
       "        [0.50753417, 0.54947884, 0.55018003, 0.61948317, 0.70802287],\n",
       "        ...,\n",
       "        [0.62420039, 0.66284836, 0.72248293, 0.85162039, 0.90237745],\n",
       "        [0.61655237, 0.63759045, 0.68281566, 0.751377  , 0.84155533],\n",
       "        [0.70539419, 0.77158273, 0.12319645, 0.33333333, 0.18191377]],\n",
       "\n",
       "       [[0.        , 0.86719834, 0.36511027, 0.17782511, 0.26753738],\n",
       "        [0.38003221, 0.394081  , 0.41545894, 0.41121495, 0.41545894],\n",
       "        [0.55018003, 0.61948317, 0.70802287, 0.68621385, 0.70476432],\n",
       "        ...,\n",
       "        [0.72248293, 0.85162039, 0.90237745, 0.95232511, 0.91935772],\n",
       "        [0.68281566, 0.751377  , 0.84155533, 0.91071565, 0.94277601],\n",
       "        [0.73651452, 0.79676259, 0.12430633, 0.34595163, 0.20715037]]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
